model:
  embedding:
    embedding_dim: 300        # fastText cc.vi.300
    padding_idx: 0
    freeze: false             # true = freeze embedding, false = fine-tune

  encoder:
    hidden_size: 256
    num_layers: 2
    dropout: 0.3

  crf:
    enabled: true

training:
  batch_size: 32
  lr: 0.001
  max_epoch: 100
  patience: 20
  optimizer: adam

data:
  pad_token: "<pad>"
  unk_token: "<unk>"
  pad_tag: "<pad_tag>"

logging:
  log_every: 100
